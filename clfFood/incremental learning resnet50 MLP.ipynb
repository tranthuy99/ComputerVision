{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e83f34d5",
   "metadata": {},
   "source": [
    "https://www.pyimagesearch.com/2019/05/27/keras-feature-extraction-on-large-datasets-with-deep-learning/?_ga=2.254520140.590795110.1633623212-2142415394.1633395405"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f03629",
   "metadata": {},
   "source": [
    "hầu hết các triển khai, bao gồm cả scikit-learn; hồi quy logistic, SVM đều yêu cầu toàn bộ tập dữ liệu được truy cập 1 lần cho việc training, tứ là nó phải fit với kích thước RAM. \n",
    "=> giải pháp: sử dụng incremental learning, cho phép đào tạo model trên một tập nhỏ dữ liệu gọi là batch.\n",
    "\n",
    "các bước:\n",
    "- load a small batch of data from dataset\n",
    "- train model on the batch\n",
    "- lặp lại qua tập dữ liệu theo batch, tiếp tục đào tạo cho đến khi hội tụ.\n",
    "\n",
    "neural network là một ví dụ của học online learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6e6d345",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "ORIG_INPUT_BASE = 'Food-5K'\n",
    "BASE_PATH = 'dataset'\n",
    "TRAIN = 'training'\n",
    "TEST = 'evaluation'\n",
    "VAL = 'validation'\n",
    "\n",
    "CLASSES = ['non_food', 'food']\n",
    "BATCH_SIZE = 32\n",
    "LE_PATH = os.path.join('output', 'le.pickle')\n",
    "BASE_CSV_PATH = 'output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3f92f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "567fc3a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94773248/94765736 [==============================] - 25s 0us/step\n"
     ]
    }
   ],
   "source": [
    "model = ResNet50(weights='imagenet', include_top=False)\n",
    "le = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60f9cb58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] preprocessing training split...\n",
      "[INFO] processing batch 1/94\n",
      "[INFO] processing batch 2/94\n",
      "[INFO] processing batch 3/94\n",
      "[INFO] processing batch 4/94\n",
      "[INFO] processing batch 5/94\n",
      "[INFO] processing batch 6/94\n",
      "[INFO] processing batch 7/94\n",
      "[INFO] processing batch 8/94\n",
      "[INFO] processing batch 9/94\n",
      "[INFO] processing batch 10/94\n",
      "[INFO] processing batch 11/94\n",
      "[INFO] processing batch 12/94\n",
      "[INFO] processing batch 13/94\n",
      "[INFO] processing batch 14/94\n",
      "[INFO] processing batch 15/94\n",
      "[INFO] processing batch 16/94\n",
      "[INFO] processing batch 17/94\n",
      "[INFO] processing batch 18/94\n",
      "[INFO] processing batch 19/94\n",
      "[INFO] processing batch 20/94\n",
      "[INFO] processing batch 21/94\n",
      "[INFO] processing batch 22/94\n",
      "[INFO] processing batch 23/94\n",
      "[INFO] processing batch 24/94\n",
      "[INFO] processing batch 25/94\n",
      "[INFO] processing batch 26/94\n",
      "[INFO] processing batch 27/94\n",
      "[INFO] processing batch 28/94\n",
      "[INFO] processing batch 29/94\n",
      "[INFO] processing batch 30/94\n",
      "[INFO] processing batch 31/94\n",
      "[INFO] processing batch 32/94\n",
      "[INFO] processing batch 33/94\n",
      "[INFO] processing batch 34/94\n",
      "[INFO] processing batch 35/94\n",
      "[INFO] processing batch 36/94\n",
      "[INFO] processing batch 37/94\n",
      "[INFO] processing batch 38/94\n",
      "[INFO] processing batch 39/94\n",
      "[INFO] processing batch 40/94\n",
      "[INFO] processing batch 41/94\n",
      "[INFO] processing batch 42/94\n",
      "[INFO] processing batch 43/94\n",
      "[INFO] processing batch 44/94\n",
      "[INFO] processing batch 45/94\n",
      "[INFO] processing batch 46/94\n",
      "[INFO] processing batch 47/94\n",
      "[INFO] processing batch 48/94\n",
      "[INFO] processing batch 49/94\n",
      "[INFO] processing batch 50/94\n",
      "[INFO] processing batch 51/94\n",
      "[INFO] processing batch 52/94\n",
      "[INFO] processing batch 53/94\n",
      "[INFO] processing batch 54/94\n",
      "[INFO] processing batch 55/94\n",
      "[INFO] processing batch 56/94\n",
      "[INFO] processing batch 57/94\n",
      "[INFO] processing batch 58/94\n",
      "[INFO] processing batch 59/94\n",
      "[INFO] processing batch 60/94\n",
      "[INFO] processing batch 61/94\n",
      "[INFO] processing batch 62/94\n",
      "[INFO] processing batch 63/94\n",
      "[INFO] processing batch 64/94\n",
      "[INFO] processing batch 65/94\n",
      "[INFO] processing batch 66/94\n",
      "[INFO] processing batch 67/94\n",
      "[INFO] processing batch 68/94\n",
      "[INFO] processing batch 69/94\n",
      "[INFO] processing batch 70/94\n",
      "[INFO] processing batch 71/94\n",
      "[INFO] processing batch 72/94\n",
      "[INFO] processing batch 73/94\n",
      "[INFO] processing batch 74/94\n",
      "[INFO] processing batch 75/94\n",
      "[INFO] processing batch 76/94\n",
      "[INFO] processing batch 77/94\n",
      "[INFO] processing batch 78/94\n",
      "[INFO] processing batch 79/94\n",
      "[INFO] processing batch 80/94\n",
      "[INFO] processing batch 81/94\n",
      "[INFO] processing batch 82/94\n",
      "[INFO] processing batch 83/94\n",
      "[INFO] processing batch 84/94\n",
      "[INFO] processing batch 85/94\n",
      "[INFO] processing batch 86/94\n",
      "[INFO] processing batch 87/94\n",
      "[INFO] processing batch 88/94\n",
      "[INFO] processing batch 89/94\n",
      "[INFO] processing batch 90/94\n",
      "[INFO] processing batch 91/94\n",
      "[INFO] processing batch 92/94\n",
      "[INFO] processing batch 93/94\n",
      "[INFO] processing batch 94/94\n",
      "[INFO] preprocessing evaluation split...\n",
      "[INFO] processing batch 1/32\n",
      "[INFO] processing batch 2/32\n",
      "[INFO] processing batch 3/32\n",
      "[INFO] processing batch 4/32\n",
      "[INFO] processing batch 5/32\n",
      "[INFO] processing batch 6/32\n",
      "[INFO] processing batch 7/32\n",
      "[INFO] processing batch 8/32\n",
      "[INFO] processing batch 9/32\n",
      "[INFO] processing batch 10/32\n",
      "[INFO] processing batch 11/32\n",
      "[INFO] processing batch 12/32\n",
      "[INFO] processing batch 13/32\n",
      "[INFO] processing batch 14/32\n",
      "[INFO] processing batch 15/32\n",
      "[INFO] processing batch 16/32\n",
      "[INFO] processing batch 17/32\n",
      "[INFO] processing batch 18/32\n",
      "[INFO] processing batch 19/32\n",
      "[INFO] processing batch 20/32\n",
      "[INFO] processing batch 21/32\n",
      "[INFO] processing batch 22/32\n",
      "[INFO] processing batch 23/32\n",
      "[INFO] processing batch 24/32\n",
      "[INFO] processing batch 25/32\n",
      "[INFO] processing batch 26/32\n",
      "[INFO] processing batch 27/32\n",
      "[INFO] processing batch 28/32\n",
      "[INFO] processing batch 29/32\n",
      "[INFO] processing batch 30/32\n",
      "[INFO] processing batch 31/32\n",
      "[INFO] processing batch 32/32\n",
      "[INFO] preprocessing validation split...\n",
      "[INFO] processing batch 1/32\n",
      "[INFO] processing batch 2/32\n",
      "[INFO] processing batch 3/32\n",
      "[INFO] processing batch 4/32\n",
      "[INFO] processing batch 5/32\n",
      "[INFO] processing batch 6/32\n",
      "[INFO] processing batch 7/32\n",
      "[INFO] processing batch 8/32\n",
      "[INFO] processing batch 9/32\n",
      "[INFO] processing batch 10/32\n",
      "[INFO] processing batch 11/32\n",
      "[INFO] processing batch 12/32\n",
      "[INFO] processing batch 13/32\n",
      "[INFO] processing batch 14/32\n",
      "[INFO] processing batch 15/32\n",
      "[INFO] processing batch 16/32\n",
      "[INFO] processing batch 17/32\n",
      "[INFO] processing batch 18/32\n",
      "[INFO] processing batch 19/32\n",
      "[INFO] processing batch 20/32\n",
      "[INFO] processing batch 21/32\n",
      "[INFO] processing batch 22/32\n",
      "[INFO] processing batch 23/32\n",
      "[INFO] processing batch 24/32\n",
      "[INFO] processing batch 25/32\n",
      "[INFO] processing batch 26/32\n",
      "[INFO] processing batch 27/32\n",
      "[INFO] processing batch 28/32\n",
      "[INFO] processing batch 29/32\n",
      "[INFO] processing batch 30/32\n",
      "[INFO] processing batch 31/32\n",
      "[INFO] processing batch 32/32\n"
     ]
    }
   ],
   "source": [
    "for tp in (TRAIN, TEST, VAL):\n",
    "    print(f'[INFO] preprocessing {tp} split...')\n",
    "    p = os.path.join(BASE_PATH, tp)\n",
    "    pp = [os.path.join(p, lb) for lb in CLASSES] \n",
    "    imagePaths = [os.path.join(p, f) for p in pp for f in os.listdir(p)]\n",
    "    random.shuffle(imagePaths)\n",
    "    labels = [p.split(os.path.sep)[-2] for p in imagePaths]\n",
    "\n",
    "    if le is None:\n",
    "        le = LabelEncoder()\n",
    "        le.fit(labels)\n",
    "    \n",
    "    csvPath = os.path.join(BASE_CSV_PATH, f'{tp}.csv')\n",
    "    if not os.path.exists(BASE_CSV_PATH):\n",
    "        os.makedirs(BASE_CSV_PATH)\n",
    "    csv = open(csvPath, 'w')\n",
    "    for (b, i) in enumerate(range(0, len(imagePaths), BATCH_SIZE)):\n",
    "        print(f'[INFO] processing batch {b+1}/{int(np.ceil(len(imagePaths)/BATCH_SIZE))}')\n",
    "        batchPaths = imagePaths[i:i+BATCH_SIZE]\n",
    "        batchLabels = le.transform(labels[i:i+BATCH_SIZE])\n",
    "        batchImages = []\n",
    "        for batchPath in batchPaths:\n",
    "            image = load_img(batchPath, target_size=(224, 224))\n",
    "            image = img_to_array(image)\n",
    "            image = np.expand_dims(image, axis=0)\n",
    "            image = preprocess_input(image)\n",
    "            batchImages.append(image)\n",
    "        batchImages = np.vstack(batchImages)\n",
    "        features = model.predict(batchImages, batch_size=BATCH_SIZE)\n",
    "        features = features.reshape((features.shape[0], 7*7*2048))\n",
    "        for (label, vec) in zip(batchLabels, features):\n",
    "            vec = ','.join([str(v) for v in vec])\n",
    "            csv.write(f'{label}, {vec}\\n')\n",
    "    csv.close()\n",
    "f = open(LE_PATH, 'wb')\n",
    "f.write(pickle.dumps(le))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f643d2ab",
   "metadata": {},
   "source": [
    "# implement the incremental learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4720b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "82fc8558",
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_feature_generator(inputPath, bs, numClasses, mode='train'):\n",
    "    f = open(inputPath, 'r')\n",
    "    f.seek(0)\n",
    "    while True:\n",
    "        data = []\n",
    "        labels = []\n",
    "        while(len(data)<bs):\n",
    "            row = f.readline()\n",
    "            if row =='':\n",
    "                f.seek(0)\n",
    "                row = f.readline()\n",
    "                if mode=='test':\n",
    "                    break\n",
    "            row = row.strip().split(',')\n",
    "            label = row[0]\n",
    "            label = to_categorical(label, num_classes= numClasses) #one hot vector\n",
    "            features = np.array(row[1:], dtype='float')\n",
    "            \n",
    "            data.append(features)\n",
    "            labels.append(label)\n",
    "        yield(np.array(data), np.array(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fab0619b",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = pickle.loads(open(LE_PATH, 'rb').read())\n",
    "trainPath = 'output/training.csv'\n",
    "testPath = 'output/evaluation.csv'\n",
    "valPath = 'output/validation.csv'\n",
    "totalTrain = sum([1 for l in open(trainPath)])\n",
    "totalVal = sum([1 for l in open(valPath)])\n",
    "testLabels = [int(row.strip().split(',')[0]) for row in open(testPath)]\n",
    "totalTest = len(testLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "31bce384",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainGen = csv_feature_generator(trainPath, BATCH_SIZE, len(CLASSES), mode='train')\n",
    "testGen = csv_feature_generator(testPath, BATCH_SIZE, len(CLASSES), mode='test')\n",
    "valGen = csv_feature_generator(valPath, BATCH_SIZE, len(CLASSES), mode='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "618dc3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 256)               25690368  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                4112      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 34        \n",
      "=================================================================\n",
      "Total params: 25,694,514\n",
      "Trainable params: 25,694,514\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(256, activation='relu', input_shape=(7*7*2048,)))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(len(CLASSES), activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974cf749",
   "metadata": {},
   "source": [
    "a good rule os thumb is to take the square root of the previous number of nodes in the layer and then find the closest power of 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e2f46e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = SGD(learning_rate=1e-3, momentum=0.9, decay=1e-3/25)\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e615a5ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training simple network...\n",
      "Epoch 1/10\n",
      "93/93 [==============================] - 128s 1s/step - loss: 0.0967 - accuracy: 0.9691 - val_loss: 0.0397 - val_accuracy: 0.9889\n",
      "Epoch 2/10\n",
      "93/93 [==============================] - 126s 1s/step - loss: 0.0072 - accuracy: 0.9983 - val_loss: 0.0386 - val_accuracy: 0.9889\n",
      "Epoch 3/10\n",
      "93/93 [==============================] - 126s 1s/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.0416 - val_accuracy: 0.9879\n",
      "Epoch 4/10\n",
      "93/93 [==============================] - 127s 1s/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0429 - val_accuracy: 0.9879\n",
      "Epoch 5/10\n",
      "93/93 [==============================] - 130s 1s/step - loss: 9.9833e-04 - accuracy: 1.0000 - val_loss: 0.0445 - val_accuracy: 0.9879\n",
      "Epoch 6/10\n",
      "93/93 [==============================] - 126s 1s/step - loss: 7.2801e-04 - accuracy: 1.0000 - val_loss: 0.0450 - val_accuracy: 0.9879\n",
      "Epoch 7/10\n",
      "93/93 [==============================] - 128s 1s/step - loss: 6.8710e-04 - accuracy: 1.0000 - val_loss: 0.0454 - val_accuracy: 0.9889\n",
      "Epoch 8/10\n",
      "93/93 [==============================] - 124s 1s/step - loss: 6.5882e-04 - accuracy: 1.0000 - val_loss: 0.0459 - val_accuracy: 0.9889\n",
      "Epoch 9/10\n",
      "93/93 [==============================] - 125s 1s/step - loss: 6.3568e-04 - accuracy: 1.0000 - val_loss: 0.0463 - val_accuracy: 0.9889\n",
      "Epoch 10/10\n",
      "93/93 [==============================] - 126s 1s/step - loss: 6.2010e-04 - accuracy: 1.0000 - val_loss: 0.0469 - val_accuracy: 0.9879\n"
     ]
    }
   ],
   "source": [
    "print('[INFO] training simple network...')\n",
    "H = model.fit(x=trainGen, steps_per_epoch=(totalTrain//BATCH_SIZE),\n",
    "             validation_data=valGen,\n",
    "             validation_steps=(totalVal//BATCH_SIZE),\n",
    "             epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6ccde59c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] evaluate network...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        food       0.99      0.99      0.99       500\n",
      "    non_food       0.99      0.99      0.99       500\n",
      "\n",
      "    accuracy                           0.99      1000\n",
      "   macro avg       0.99      0.99      0.99      1000\n",
      "weighted avg       0.99      0.99      0.99      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('[INFO] evaluate network...')\n",
    "predIdx = model.predict(x=testGen, steps=np.ceil(totalTest/BATCH_SIZE))\n",
    "predIdxx = np.argmax(predIdx, axis=1)\n",
    "print(classification_report(testLabels, predIdxx, target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1beba8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
